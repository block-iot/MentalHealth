{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nd = pd.read_csv('numeric_data.csv')\n",
    "print(nd.shape)\n",
    "\n",
    "output = [\"stais1\",\"stais3\",\"stais4\",\"stais5\",\"stais6\",\"stais7\",\"stais8\",\"stais9\",\"stais10\",\"stais11\",\"stais12\",\"stais13\",\n",
    "          \"stais14\",\"stais15\",\"stais16\",\"stais17\",\"stais18\",\"stais19\",\"stais20\",\n",
    "          \"sds_1\",\"sds_3\",\"sds_4\",\"stait5\",\"sds_6\",\"sds_7\",\"sds_8\",\"sds_9\",\"sds_10\",\"sds_11\",\"sds_12\",\"sds_13\",\n",
    "          \"sds_14\",\"sds_15\",\"sds_16\",\"sds_17\",\"sds_18\",\"sds_19\",\"sds_20\"]\n",
    "target = ['stai_s_score', 'sds_score', 'stai_t_score']\n",
    "\n",
    "data_y = nd.loc[:,output]\n",
    "data_x = nd\n",
    "\n",
    "columns = nd.columns.values\n",
    "\n",
    "for col in columns:\n",
    "    if col in output:\n",
    "        data_x = data_x.drop([col],axis = 1)\n",
    "        #print(col)\n",
    "    elif col in target:\n",
    "        data_x = data_x.drop([col],axis = 1)\n",
    "        #print(col)\n",
    "    elif nd[col].isna().sum() != 0:\n",
    "        data_x = data_x.drop([col],axis = 1)\n",
    "        #print(col)\n",
    "    elif nd[col].dtypes == object:\n",
    "        data_x = data_x.drop([col],axis = 1)\n",
    "        #print(nd[col].dtypes)\n",
    "\n",
    "    '''\n",
    "    elif not nd[col].dtypes is np.dtype('int64'):\n",
    "        if not nd[col].dtypes is np.dtype('float64'):\n",
    "            data_x = data_x.drop([col],axis = 1)\n",
    "            print(nd[col].dtypes)\n",
    "    ''' \n",
    "\n",
    "#data_y = nd.loc[:,[\"stai_s_score\"]]\n",
    "\n",
    "features_names = data_x.columns.values\n",
    "print(len(features_names))\n",
    "print(features_names)\n",
    "\n",
    "\n",
    "data_x = np.array(data_x, dtype=np.float32)\n",
    "#print(data_x)\n",
    "data_y = np.array(data_y, dtype=np.float32)\n",
    "#print(data_y)\n",
    "\n",
    "X = data_x[:1316]\n",
    "y = data_y[:1316]\n",
    "X_test = data_x[1316:]\n",
    "y_test = data_y[1316:]\n",
    "\n",
    "#print(data_x.shape)\n",
    "#print(data_y.shape)\n",
    "#print(data_t.shape)\n",
    "\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x1 = np.delete(X,slice(263),0)\n",
    "x2 = np.delete(X,slice(263,526),0)\n",
    "x3 = np.delete(X,slice(526,790),0)\n",
    "x4 = np.delete(X,slice(790,1053),0)\n",
    "x5 = X[:1053]\n",
    "\n",
    "\n",
    "y1 = np.delete(y,slice(263),0)\n",
    "y2 = np.delete(y,slice(263,526),0)\n",
    "y3 = np.delete(y,slice(526,790),0)\n",
    "y4 = np.delete(y,slice(790,1053),0)\n",
    "y5 = y[:1053]\n",
    "\n",
    "X_groups = [x1,x2,x3,x4,x5]\n",
    "y_groups = [y1,y2,y3,y4,y5]\n",
    "X_vali = [X[:263],X[263:526],X[526:790],X[790:1053],X[1053:]]\n",
    "y_vali = [y[:263],y[263:526],y[526:790],y[790:1053],y[1053:]]\n",
    "\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 100\n",
    "output_size = y.shape[1]\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Training\n",
    "mse = 0\n",
    "for i in range(5):\n",
    "    X_tensor = torch.tensor(X_groups[i])\n",
    "    y_tensor = torch.tensor(y_groups[i])\n",
    "    \n",
    "    X_validation = torch.tensor(X_vali[i])\n",
    "    y_validation = torch.tensor(y_vali[i])\n",
    "    \n",
    "    \n",
    "    model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        outputs = model(X_tensor)\n",
    "        loss = criterion(outputs, y_tensor)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    predictions = model(X_validation)\n",
    "    loss = criterion(predictions, y_validation)\n",
    "    print(\"Validation loss:\",str(loss.item()))\n",
    "    mse += loss.item()\n",
    "\n",
    "CV_k = mse / 5\n",
    "print(CV_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "\n",
    "model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "\n",
    "#print(predictions)\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754742db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(data):\n",
    "    return [sum(((5-data[0]),(data[1]),(data[2]),(5-data[3]),(data[4]),(data[5]),(5-data[6]),(data[7]),\n",
    "               (5-data[8]),(5-data[9]),(data[10]),(data[11]),(data[12]),(5-data[13]),(5-data[14]),\n",
    "               (data[15]),(data[16]),(5-data[17]),(5-data[18]))),\n",
    "            sum(((data[19]),(data[20]),(data[21]),(5-data[22]),(5-data[23]),(data[24]),(data[25]),(data[26]),\n",
    "               (data[27]),(5-data[28]),(5-data[29]),(data[30]),(5-data[31]),(data[32]),(5-data[33]),\n",
    "               (5-data[34]),(5-data[35]),(data[36]),(5-data[37])))\n",
    "           ]\n",
    "\n",
    "\n",
    "#print([int(i+0.5) for i in predictions[0]])\n",
    "#print(y_test[0].shape)\n",
    "\n",
    "predict = predictions.tolist()\n",
    "#print(predict)\n",
    "\n",
    "p_scores = [scores(i) for i in predict]\n",
    "y_scores = [scores(i) for i in y_test]\n",
    "#print(p_scores)\n",
    "\n",
    "p_sc = [[int(i[0]+0.5),int(i[1]+0.5)] for i in p_scores]\n",
    "y_sc = [[int(j[0]),int(j[1])] for j in y_scores]\n",
    "#print(p_sc)\n",
    "#print()\n",
    "#print(y_sc)\n",
    "\n",
    "\n",
    "p_s = torch.tensor(p_scores)\n",
    "y_s = torch.tensor(y_scores)\n",
    "\n",
    "\n",
    "l = criterion(p_s, y_s)\n",
    "print(\"\\nMSEloss:\")\n",
    "print(l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = pd.read_csv(\"t1_data_dictionary.csv\")\n",
    "data_dictionary[\"Field Type\"].unique()\n",
    "\n",
    "data_dictionary.drop_duplicates(subset=\"Field Type\")\n",
    "\n",
    "\n",
    "categorical_df = data_dictionary[data_dictionary[\"Field Type\"].isin([\"radio\", \"dropdown\", \"checkbox\", \"slider\"])]\n",
    "categorical_cols = set(categorical_df[\"Variable / Field Name\"].tolist())\n",
    "\n",
    "categorical_indices = [i for i in range(len(columns)) if columns[i] in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "def predict_f(data, model = model):\n",
    "    X_tensor = torch.FloatTensor(data)\n",
    "    predictions = model(X_tensor)\n",
    "    predict = predictions.tolist()\n",
    "    #print(predict)\n",
    "    p_scores = np.array([scores(i) for i in predict])\n",
    "    return p_scores\n",
    "\n",
    "testD = X_test[:1]\n",
    "p = predict_f(data = testD, model = model)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0688f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['stai_s_score', 'sds_score']\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(training_data = data_x, \n",
    "                                                   mode = \"regression\",\n",
    "                                                   feature_names = features_names,\n",
    "                                                   categorical_features = categorical_cols,\n",
    "                                                   class_names = targets, \n",
    "                                                   discretize_continuous = True)\n",
    "\n",
    "\n",
    "count = {}\n",
    "for i in range(100):\n",
    "    exp = explainer.explain_instance(data_row = data_x[1], predict_fn = predict_f)\n",
    "    #exp.show_in_notebook(show_table = True)\n",
    "    a = exp.as_list()\n",
    "    #print(a)\n",
    "\n",
    "    for i in a:\n",
    "        key = i[0].split(' ')[0]\n",
    "        if key in count:\n",
    "            count[key] += 1\n",
    "        else:\n",
    "            count[key] = 1\n",
    "\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac757c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(count.items(), key = lambda kv:(kv[1], kv[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77cd5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
